{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07522249",
   "metadata": {},
   "source": [
    "# Entrenamiento de GAN para Colorización Automática de Imágenes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1235deb6",
   "metadata": {},
   "source": [
    "Este notebook presenta la implementación completa del sistema de entrenamiento para una Red Generativa Adversarial (GAN) especializada en colorización automática de imágenes en escala de grises. El proyecto utiliza una arquitectura tipo Pix2Pix que combina un generador basado en U-Net con un discriminador PatchGAN, operando en el espacio de color LAB para optimizar la predicción de los canales cromáticos (a, b) a partir del canal de luminancia (L). El sistema incluye un framework robusto de entrenamiento con checkpointing automático, seguimiento de métricas en tiempo real mediante TensorBoard, schedulers de learning rate adaptativos, y generación periódica de muestras visuales para monitorear la calidad de las imágenes coloreadas. Esta implementación está diseñada para ser escalable, reproducible y fácil de experimentar con diferentes configuraciones de hiperparámetros.Reintentar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bada1c8",
   "metadata": {},
   "source": [
    "# 1. Introducción al Proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87abd40",
   "metadata": {},
   "source": [
    "# 2. Importación de Librerías y Dependencias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27cb2df",
   "metadata": {},
   "source": [
    "Todas las librerías necesarias para el proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2472429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import lab2rgb\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009918a",
   "metadata": {},
   "source": [
    "# 3. Configuración del Entrenamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6b3ba",
   "metadata": {},
   "source": [
    "## 3.1 Clase TrainingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b26e6d",
   "metadata": {},
   "source": [
    "Configuración centralizada de todos los hiperparámetros del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    \"\"\"Configuración centralizada del entrenamiento\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Directorios\n",
    "        self.data_dir = \"./ImagesProcessed/ImagesProcessed/color\"\n",
    "        self.checkpoint_dir = \"checkpoints\"\n",
    "        self.samples_dir = \"training_samples\"\n",
    "        self.logs_dir = \"logs\"\n",
    "        \n",
    "        # Hiperparámetros de entrenamiento\n",
    "        self.num_epochs = 100\n",
    "        self.batch_size = 12\n",
    "        self.num_workers = 4\n",
    "        \n",
    "        # Optimizadores\n",
    "        self.lr_generator = 2e-4\n",
    "        self.lr_discriminator = 2e-4\n",
    "        self.beta1 = 0.5\n",
    "        self.beta2 = 0.999\n",
    "        \n",
    "        # Schedulers\n",
    "        self.use_scheduler = True\n",
    "        self.scheduler_type = \"step\"  # \"step\", \"cosine\", \"plateau\"\n",
    "        self.step_size = 30\n",
    "        self.gamma = 0.5\n",
    "        \n",
    "        # Pérdidas\n",
    "        self.lambda_l1 = 100.0  # Peso de la pérdida L1\n",
    "        \n",
    "        # Checkpointing\n",
    "        self.save_every = 5  # Guardar cada N épocas\n",
    "        self.keep_last_n = 3  # Mantener últimos N checkpoints\n",
    "        \n",
    "        # Validación y monitoreo\n",
    "        self.validate_every = 1\n",
    "        self.log_every = 10  # Log cada N batches\n",
    "        self.sample_every = 1  # Guardar samples cada N épocas\n",
    "        self.num_samples = 8\n",
    "        \n",
    "        # Dataset\n",
    "        self.num_classes = 8\n",
    "        self.image_size = 128\n",
    "        \n",
    "        # Device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convierte la configuración a diccionario\"\"\"\n",
    "        return {k: str(v) if isinstance(v, (Path, torch.device)) else v \n",
    "                for k, v in self.__dict__.items()}\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Guarda la configuración en JSON\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04866857",
   "metadata": {},
   "source": [
    "## 3.2 Mapeo de Categorías\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e305a232",
   "metadata": {},
   "source": [
    "Diccionario que relaciona índices numéricos con nombres de categorías del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_MAP = {\n",
    "    1: \"airplane\",\n",
    "    2: \"car\",\n",
    "    3: \"cat\",\n",
    "    4: \"dog\",\n",
    "    5: \"flower\",\n",
    "    6: \"fruit\",\n",
    "    7: \"motorbike\",\n",
    "    8: \"person\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85e8a4",
   "metadata": {},
   "source": [
    "# 4. Sistema de Gestión de Checkpoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621166cd",
   "metadata": {},
   "source": [
    "Maneja el guardado, carga y limpieza de checkpoints durante el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db88fa",
   "metadata": {},
   "source": [
    "### 4.1.2 Método _cleanup_old_checkpoints\n",
    "Elimina checkpoints antiguos manteniendo solo los últimos N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb07d2",
   "metadata": {},
   "source": [
    "### 4.1.3 Método load_checkpoint\n",
    "Carga un checkpoint previo para reanudar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointManager:\n",
    "    \"\"\"Gestiona guardado y carga de checkpoints\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: str, keep_last_n: int = 3):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.keep_last_n = keep_last_n\n",
    "        \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        epoch: int,\n",
    "        generator: nn.Module,\n",
    "        discriminator: nn.Module,\n",
    "        optim_G: optim.Optimizer,\n",
    "        optim_D: optim.Optimizer,\n",
    "        scheduler_G: Optional[optim.lr_scheduler._LRScheduler] = None,\n",
    "        scheduler_D: Optional[optim.lr_scheduler._LRScheduler] = None,\n",
    "        metrics: Optional[Dict] = None,\n",
    "        is_best: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Guarda un checkpoint completo del entrenamiento\n",
    "        \n",
    "        Args:\n",
    "            epoch: Época actual\n",
    "            generator: Modelo generador\n",
    "            discriminator: Modelo discriminador\n",
    "            optim_G: Optimizador del generador\n",
    "            optim_D: Optimizador del discriminador\n",
    "            scheduler_G: Scheduler del generador (opcional)\n",
    "            scheduler_D: Scheduler del discriminador (opcional)\n",
    "            metrics: Métricas a guardar (opcional)\n",
    "            is_best: Si es el mejor modelo hasta ahora\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optim_G_state_dict': optim_G.state_dict(),\n",
    "            'optim_D_state_dict': optim_D.state_dict(),\n",
    "            'metrics': metrics or {},\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if scheduler_G is not None:\n",
    "            checkpoint['scheduler_G_state_dict'] = scheduler_G.state_dict()\n",
    "        if scheduler_D is not None:\n",
    "            checkpoint['scheduler_D_state_dict'] = scheduler_D.state_dict()\n",
    "        \n",
    "        # Guardar checkpoint regular\n",
    "        checkpoint_path = self.checkpoint_dir / f\"checkpoint_epoch_{epoch:04d}.pth\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"✓ Checkpoint guardado: {checkpoint_path}\")\n",
    "        \n",
    "        # Guardar como último checkpoint\n",
    "        latest_path = self.checkpoint_dir / \"checkpoint_latest.pth\"\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        # Si es el mejor, guardarlo también\n",
    "        if is_best:\n",
    "            best_path = self.checkpoint_dir / \"checkpoint_best.pth\"\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"★ Mejor modelo guardado: {best_path}\")\n",
    "        \n",
    "        # Limpiar checkpoints antiguos\n",
    "        self._cleanup_old_checkpoints()\n",
    "    \n",
    "    def _cleanup_old_checkpoints(self):\n",
    "        \"\"\"Elimina checkpoints antiguos, manteniendo solo los últimos N\"\"\"\n",
    "        checkpoints = sorted(\n",
    "            [f for f in self.checkpoint_dir.glob(\"checkpoint_epoch_*.pth\")],\n",
    "            key=lambda x: x.stat().st_mtime\n",
    "        )\n",
    "        \n",
    "        if len(checkpoints) > self.keep_last_n:\n",
    "            for old_checkpoint in checkpoints[:-self.keep_last_n]:\n",
    "                old_checkpoint.unlink()\n",
    "                print(f\"✗ Checkpoint antiguo eliminado: {old_checkpoint.name}\")\n",
    "    \n",
    "    def load_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: str,\n",
    "        generator: nn.Module,\n",
    "        discriminator: nn.Module,\n",
    "        optim_G: optim.Optimizer,\n",
    "        optim_D: optim.Optimizer,\n",
    "        scheduler_G: Optional[optim.lr_scheduler._LRScheduler] = None,\n",
    "        scheduler_D: Optional[optim.lr_scheduler._LRScheduler] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Carga un checkpoint\n",
    "        \n",
    "        Returns:\n",
    "            Dict con información del checkpoint cargado\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        \n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        optim_G.load_state_dict(checkpoint['optim_G_state_dict'])\n",
    "        optim_D.load_state_dict(checkpoint['optim_D_state_dict'])\n",
    "        \n",
    "        if scheduler_G is not None and 'scheduler_G_state_dict' in checkpoint:\n",
    "            scheduler_G.load_state_dict(checkpoint['scheduler_G_state_dict'])\n",
    "        if scheduler_D is not None and 'scheduler_D_state_dict' in checkpoint:\n",
    "            scheduler_D.load_state_dict(checkpoint['scheduler_D_state_dict'])\n",
    "        \n",
    "        print(f\"✓ Checkpoint cargado desde época {checkpoint['epoch']}\")\n",
    "        return checkpoint\n",
    "    \n",
    "    def find_latest_checkpoint(self) -> Optional[str]:\n",
    "        \"\"\"Encuentra el último checkpoint guardado\"\"\"\n",
    "        latest_path = self.checkpoint_dir / \"checkpoint_latest.pth\"\n",
    "        if latest_path.exists():\n",
    "            return str(latest_path)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70110700",
   "metadata": {},
   "source": [
    "## 5.1 Clase MetricsTracker\n",
    " Registra, visualiza y guarda todas las métricas del entrenamiento\n",
    "\n",
    "### 5.1.1 Inicialización y Configuración\n",
    " Setup de TensorBoard y estructuras para almacenar métricas\n",
    "\n",
    "### 5.1.2 Método update\n",
    " Actualiza las métricas en cada época (train o validación)\n",
    "\n",
    "### 5.1.3 Método log_learning_rates\n",
    " Registra los learning rates del generador y discriminador\n",
    "\n",
    "### 5.1.4 Método is_best_model\n",
    " Determina si el modelo actual es el mejor hasta ahora\n",
    "\n",
    "### 5.1.5 Método save_history\n",
    " Guarda el historial completo de métricas en formato JSON\n",
    "\n",
    "### 5.1.6 Método plot_history\n",
    " Genera gráficas de evolución de pérdidas durante el entrenamiento\n",
    "\n",
    "### 5.1.7 Método log_samples\n",
    " Registra imágenes de muestra en TensorBoard\n",
    "\n",
    "### 5.1.8 Método close\n",
    " Cierra el escritor de TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracker:\n",
    "    \"\"\"Rastrea y registra métricas del entrenamiento\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir: str):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # TensorBoard\n",
    "        self.writer = SummaryWriter(log_dir=str(self.log_dir))\n",
    "        \n",
    "        # Historial de métricas\n",
    "        self.history = {\n",
    "            'train_loss_D': [],\n",
    "            'train_loss_G': [],\n",
    "            'train_loss_G_gan': [],\n",
    "            'train_loss_G_l1': [],\n",
    "            'val_loss_D': [],\n",
    "            'val_loss_G': [],\n",
    "            'learning_rates': {'G': [], 'D': []}\n",
    "        }\n",
    "        \n",
    "        # Mejor pérdida para early stopping\n",
    "        self.best_val_loss_G = float('inf')\n",
    "        \n",
    "    def update(\n",
    "        self,\n",
    "        epoch: int,\n",
    "        metrics: Dict[str, float],\n",
    "        phase: str = 'train'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Actualiza métricas\n",
    "        \n",
    "        Args:\n",
    "            epoch: Época actual\n",
    "            metrics: Diccionario con métricas\n",
    "            phase: 'train' o 'val'\n",
    "        \"\"\"\n",
    "        for key, value in metrics.items():\n",
    "            full_key = f\"{phase}_{key}\"\n",
    "            if full_key in self.history:\n",
    "                self.history[full_key].append(value)\n",
    "            \n",
    "            # Log a TensorBoard\n",
    "            self.writer.add_scalar(f\"{phase}/{key}\", value, epoch)\n",
    "    \n",
    "    def log_learning_rates(self, epoch: int, lr_G: float, lr_D: float):\n",
    "        \"\"\"Registra learning rates\"\"\"\n",
    "        self.history['learning_rates']['G'].append(lr_G)\n",
    "        self.history['learning_rates']['D'].append(lr_D)\n",
    "        self.writer.add_scalar('learning_rate/generator', lr_G, epoch)\n",
    "        self.writer.add_scalar('learning_rate/discriminator', lr_D, epoch)\n",
    "    \n",
    "    def is_best_model(self, val_loss_G: float) -> bool:\n",
    "        \"\"\"Determina si es el mejor modelo hasta ahora\"\"\"\n",
    "        if val_loss_G < self.best_val_loss_G:\n",
    "            self.best_val_loss_G = val_loss_G\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def plot_history(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Genera gráficas de las métricas\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Pérdidas del discriminador\n",
    "        if self.history['train_loss_D']:\n",
    "            axes[0, 0].plot(self.history['train_loss_D'], label='Train')\n",
    "            if self.history['val_loss_D']:\n",
    "                axes[0, 0].plot(self.history['val_loss_D'], label='Val')\n",
    "            axes[0, 0].set_title('Pérdida del Discriminador')\n",
    "            axes[0, 0].set_xlabel('Época')\n",
    "            axes[0, 0].set_ylabel('Pérdida')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True)\n",
    "        \n",
    "        # Pérdidas del generador\n",
    "        if self.history['train_loss_G']:\n",
    "            axes[0, 1].plot(self.history['train_loss_G'], label='Train Total')\n",
    "            if self.history['val_loss_G']:\n",
    "                axes[0, 1].plot(self.history['val_loss_G'], label='Val Total')\n",
    "            axes[0, 1].set_title('Pérdida del Generador')\n",
    "            axes[0, 1].set_xlabel('Época')\n",
    "            axes[0, 1].set_ylabel('Pérdida')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True)\n",
    "        \n",
    "        # Componentes de pérdida del generador\n",
    "        if self.history['train_loss_G_gan']:\n",
    "            axes[1, 0].plot(self.history['train_loss_G_gan'], label='GAN Loss')\n",
    "            axes[1, 0].plot(self.history['train_loss_G_l1'], label='L1 Loss')\n",
    "            axes[1, 0].set_title('Componentes de Pérdida del Generador')\n",
    "            axes[1, 0].set_xlabel('Época')\n",
    "            axes[1, 0].set_ylabel('Pérdida')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True)\n",
    "        \n",
    "        # Learning rates\n",
    "        if self.history['learning_rates']['G']:\n",
    "            axes[1, 1].plot(self.history['learning_rates']['G'], label='Generator')\n",
    "            axes[1, 1].plot(self.history['learning_rates']['D'], label='Discriminator')\n",
    "            axes[1, 1].set_title('Learning Rates')\n",
    "            axes[1, 1].set_xlabel('Época')\n",
    "            axes[1, 1].set_ylabel('LR')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True)\n",
    "            axes[1, 1].set_yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"✓ Gráficas guardadas: {save_path}\")\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "        plt.close(fig)\n",
    "    \n",
    "    def save_history(self, path: str):\n",
    "        \"\"\"Guarda historial de métricas\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cierra el writer de TensorBoard\"\"\"\n",
    "        self.writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f8560",
   "metadata": {},
   "source": [
    "# 6. Generador de Muestras Visuales\n",
    "## 6.1 Clase SampleGenerator\n",
    " Crea visualizaciones comparativas de imágenes (original vs generada)\n",
    "\n",
    "\n",
    "### 6.1.1 Método lab_to_rgb\n",
    " Convierte imágenes del espacio LAB a RGB\n",
    "\n",
    "\n",
    "### 6.1.2 Método generate_samples\n",
    "Genera un grid de comparación de imágenes coloreadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f48987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_colorization_samples(\n",
    "    gray: torch.Tensor,\n",
    "    fake_ab: torch.Tensor,\n",
    "    real_ab: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    epoch: int,\n",
    "    save_dir: str,\n",
    "    max_samples: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Guarda samples de colorización para monitoreo visual\n",
    "    \n",
    "    Args:\n",
    "        gray: Imágenes en escala de grises [B, 1, H, W]\n",
    "        fake_ab: Colores generados [B, 2, H, W]\n",
    "        real_ab: Colores reales [B, 2, H, W]\n",
    "        labels: Labels one-hot [B, num_classes]\n",
    "        epoch: Época actual\n",
    "        save_dir: Directorio de guardado\n",
    "        max_samples: Número máximo de samples a guardar\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    num_samples = min(max_samples, gray.size(0))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Desnormalizar\n",
    "        L = (gray[i, 0].cpu().numpy() + 1.0) * 50.0\n",
    "        real_ab_np = real_ab[i].cpu().numpy() * 80.0\n",
    "        fake_ab_np = fake_ab[i].detach().cpu().numpy() * 80.0\n",
    "        \n",
    "        # Convertir LAB → RGB\n",
    "        def lab_to_rgb(L, ab):\n",
    "            lab = np.zeros((L.shape[0], L.shape[1], 3))\n",
    "            lab[:, :, 0] = L\n",
    "            lab[:, :, 1:] = ab.transpose(1, 2, 0)\n",
    "            rgb = np.clip(lab2rgb(lab), 0, 1)\n",
    "            return rgb\n",
    "        \n",
    "        gray_img = np.repeat((L / 100.0)[..., None], 3, axis=2)\n",
    "        real_rgb = lab_to_rgb(L, real_ab_np)\n",
    "        fake_rgb = lab_to_rgb(L, fake_ab_np)\n",
    "        \n",
    "        # Mostrar imágenes\n",
    "        imgs = [gray_img, real_rgb, fake_rgb]\n",
    "        titles = [\"Entrada (Grayscale)\", \"Ground Truth\", \"Generado\"]\n",
    "        \n",
    "        for j, ax in enumerate(axes[i]):\n",
    "            ax.imshow(imgs[j])\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(titles[j], fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Añadir etiqueta de categoría\n",
    "        label_idx = labels[i].argmax().item() + 1\n",
    "        category = CATEGORY_MAP.get(label_idx, f\"Class {label_idx}\")\n",
    "        axes[i, 0].set_ylabel(\n",
    "            category,\n",
    "            rotation=0,\n",
    "            labelpad=50,\n",
    "            fontsize=8,\n",
    "            va='center',\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    plt.suptitle(f\"Época {epoch}\", fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f\"samples_epoch_{epoch:04d}.png\")\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"✓ Samples guardados: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3720ffab",
   "metadata": {},
   "source": [
    "# 7. Clase Principal: GANTrainer\n",
    "Sistema completo de entrenamiento que coordina modelos, optimizadores, pérdidas y el ciclo de entrenamiento/validación\n",
    "\n",
    "## 7.1 Inicialización y Configuración\n",
    "Constructor e inicialización completa del trainer incluyendo optimizadores, schedulers, funciones de pérdida y reanudación desde checkpoints\n",
    "\n",
    "## 7.2 Ciclo de Entrenamiento por Época (train_epoch)\n",
    "Ejecuta una época completa procesando todos los batches. Entrena discriminador (reales vs falsas), luego generador (GAN loss + L1 loss), y registra métricas\n",
    "\n",
    "## 7.3 Validación del Modelo (validate)\n",
    "Evalúa el modelo en validación sin gradientes, calcula pérdidas y genera muestras visuales para monitoreo\n",
    "\n",
    "## 7.4 Loop Principal de Entrenamiento (train)\n",
    "Orquesta el entrenamiento completo: itera épocas, alterna train/validación, actualiza learning rates, guarda checkpoints y genera reportes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf76f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANTrainer:\n",
    "    \"\"\"Clase principal para entrenar la GAN de colorización\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        generator: nn.Module,\n",
    "        discriminator: nn.Module,\n",
    "        config: TrainingConfig\n",
    "    ):\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.config = config\n",
    "        \n",
    "        # Mover modelos al device\n",
    "        self.generator.to(config.device)\n",
    "        self.discriminator.to(config.device)\n",
    "        \n",
    "        # Criterios de pérdida\n",
    "        self.criterion_gan = nn.BCEWithLogitsLoss()\n",
    "        self.criterion_l1 = nn.L1Loss()\n",
    "        \n",
    "        # Optimizadores\n",
    "        self.optim_G = optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=config.lr_generator,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        self.optim_D = optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=config.lr_discriminator,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        \n",
    "        # Schedulers\n",
    "        self.scheduler_G = None\n",
    "        self.scheduler_D = None\n",
    "        if config.use_scheduler:\n",
    "            self.scheduler_G = self._create_scheduler(self.optim_G, config)\n",
    "            self.scheduler_D = self._create_scheduler(self.optim_D, config)\n",
    "        \n",
    "        # Managers\n",
    "        self.checkpoint_manager = CheckpointManager(\n",
    "            config.checkpoint_dir,\n",
    "            keep_last_n=config.keep_last_n\n",
    "        )\n",
    "        self.metrics_tracker = MetricsTracker(config.logs_dir)\n",
    "        \n",
    "        # Estado del entrenamiento\n",
    "        self.start_epoch = 0\n",
    "        self.global_step = 0\n",
    "    \n",
    "    def _create_scheduler(\n",
    "        self,\n",
    "        optimizer: optim.Optimizer,\n",
    "        config: TrainingConfig\n",
    "    ) -> optim.lr_scheduler._LRScheduler:\n",
    "        \"\"\"Crea un learning rate scheduler\"\"\"\n",
    "        if config.scheduler_type == \"step\":\n",
    "            return optim.lr_scheduler.StepLR(\n",
    "                optimizer,\n",
    "                step_size=config.step_size,\n",
    "                gamma=config.gamma\n",
    "            )\n",
    "        elif config.scheduler_type == \"cosine\":\n",
    "            return optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=config.num_epochs\n",
    "            )\n",
    "        elif config.scheduler_type == \"plateau\":\n",
    "            return optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',\n",
    "                factor=0.5,\n",
    "                patience=10,\n",
    "                verbose=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Scheduler type '{config.scheduler_type}' no reconocido\")\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Carga un checkpoint para reanudar entrenamiento\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Ruta al checkpoint. Si es None, busca el último.\n",
    "        \"\"\"\n",
    "        if checkpoint_path is None:\n",
    "            checkpoint_path = self.checkpoint_manager.find_latest_checkpoint()\n",
    "            if checkpoint_path is None:\n",
    "                print(\"No se encontró checkpoint previo. Iniciando desde cero.\")\n",
    "                return\n",
    "        \n",
    "        checkpoint = self.checkpoint_manager.load_checkpoint(\n",
    "            checkpoint_path,\n",
    "            self.generator,\n",
    "            self.discriminator,\n",
    "            self.optim_G,\n",
    "            self.optim_D,\n",
    "            self.scheduler_G,\n",
    "            self.scheduler_D\n",
    "        )\n",
    "        \n",
    "        self.start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Reanudando entrenamiento desde época {self.start_epoch}\")\n",
    "    \n",
    "    def train_epoch(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        epoch: int\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Entrena una época completa\n",
    "        \n",
    "        Returns:\n",
    "            Dict con métricas promedio de la época\n",
    "        \"\"\"\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        metrics = {\n",
    "            'loss_D': 0.0,\n",
    "            'loss_G': 0.0,\n",
    "            'loss_G_gan': 0.0,\n",
    "            'loss_G_l1': 0.0\n",
    "        }\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Época {epoch}/{self.config.num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (gray, ab_color, labels) in enumerate(pbar):\n",
    "            # Mover datos al device\n",
    "            gray = gray.to(self.config.device)\n",
    "            ab_color = ab_color.to(self.config.device)\n",
    "            labels = labels.to(self.config.device)\n",
    "            \n",
    "            batch_size = gray.size(0)\n",
    "            \n",
    "            # ==========================================\n",
    "            # 1. Entrenar Discriminador\n",
    "            # ==========================================\n",
    "            self.optim_D.zero_grad()\n",
    "            \n",
    "            # Forward del generador (sin gradientes)\n",
    "            with torch.no_grad():\n",
    "                fake_ab = self.generator(gray, labels)\n",
    "            \n",
    "            # Discriminador sobre imágenes reales\n",
    "            real_input = torch.cat([gray, ab_color], dim=1)\n",
    "            real_pred = self.discriminator(real_input)\n",
    "            loss_D_real = self.criterion_gan(\n",
    "                real_pred,\n",
    "                torch.ones_like(real_pred)\n",
    "            )\n",
    "            \n",
    "            # Discriminador sobre imágenes falsas\n",
    "            fake_input = torch.cat([gray, fake_ab], dim=1)\n",
    "            fake_pred = self.discriminator(fake_input)\n",
    "            loss_D_fake = self.criterion_gan(\n",
    "                fake_pred,\n",
    "                torch.zeros_like(fake_pred)\n",
    "            )\n",
    "            \n",
    "            # Pérdida total del discriminador\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D.backward()\n",
    "            self.optim_D.step()\n",
    "            \n",
    "            # ==========================================\n",
    "            # 2. Entrenar Generador\n",
    "            # ==========================================\n",
    "            self.optim_G.zero_grad()\n",
    "            \n",
    "            # Forward del generador\n",
    "            fake_ab = self.generator(gray, labels)\n",
    "            \n",
    "            # Pérdida GAN\n",
    "            fake_input = torch.cat([gray, fake_ab], dim=1)\n",
    "            fake_pred = self.discriminator(fake_input)\n",
    "            loss_G_gan = self.criterion_gan(\n",
    "                fake_pred,\n",
    "                torch.ones_like(fake_pred)\n",
    "            )\n",
    "            \n",
    "            # Pérdida L1\n",
    "            loss_G_l1 = self.criterion_l1(fake_ab, ab_color) * self.config.lambda_l1\n",
    "            \n",
    "            # Pérdida total del generador\n",
    "            loss_G = loss_G_gan + loss_G_l1\n",
    "            loss_G.backward()\n",
    "            self.optim_G.step()\n",
    "            \n",
    "            # ==========================================\n",
    "            # 3. Actualizar métricas\n",
    "            # ==========================================\n",
    "            metrics['loss_D'] += loss_D.item()\n",
    "            metrics['loss_G'] += loss_G.item()\n",
    "            metrics['loss_G_gan'] += loss_G_gan.item()\n",
    "            metrics['loss_G_l1'] += loss_G_l1.item()\n",
    "            \n",
    "            # Actualizar barra de progreso\n",
    "            pbar.set_postfix({\n",
    "                'D': f\"{loss_D.item():.4f}\",\n",
    "                'G': f\"{loss_G.item():.4f}\",\n",
    "                'G_gan': f\"{loss_G_gan.item():.4f}\",\n",
    "                'G_l1': f\"{loss_G_l1.item():.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Log periódico a TensorBoard\n",
    "            if batch_idx % self.config.log_every == 0:\n",
    "                self.metrics_tracker.writer.add_scalar(\n",
    "                    'batch/loss_D',\n",
    "                    loss_D.item(),\n",
    "                    self.global_step\n",
    "                )\n",
    "                self.metrics_tracker.writer.add_scalar(\n",
    "                    'batch/loss_G',\n",
    "                    loss_G.item(),\n",
    "                    self.global_step\n",
    "                )\n",
    "            \n",
    "            self.global_step += 1\n",
    "        \n",
    "        # Promediar métricas\n",
    "        num_batches = len(train_loader)\n",
    "        metrics = {k: v / num_batches for k, v in metrics.items()}\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(\n",
    "        self,\n",
    "        val_loader: DataLoader,\n",
    "        epoch: int\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evalúa el modelo en el conjunto de validación\n",
    "        \n",
    "        Returns:\n",
    "            Dict con métricas de validación\n",
    "        \"\"\"\n",
    "        self.generator.eval()\n",
    "        self.discriminator.eval()\n",
    "        \n",
    "        metrics = {\n",
    "            'loss_D': 0.0,\n",
    "            'loss_G': 0.0,\n",
    "            'loss_G_gan': 0.0,\n",
    "            'loss_G_l1': 0.0\n",
    "        }\n",
    "        \n",
    "        # Para guardar samples\n",
    "        save_samples = True\n",
    "        \n",
    "        for batch_idx, (gray, ab_color, labels) in enumerate(val_loader):\n",
    "            gray = gray.to(self.config.device)\n",
    "            ab_color = ab_color.to(self.config.device)\n",
    "            labels = labels.to(self.config.device)\n",
    "            \n",
    "            # Forward del generador\n",
    "            fake_ab = self.generator(gray, labels)\n",
    "            \n",
    "            # Pérdida del discriminador\n",
    "            real_input = torch.cat([gray, ab_color], dim=1)\n",
    "            fake_input = torch.cat([gray, fake_ab], dim=1)\n",
    "            \n",
    "            real_pred = self.discriminator(real_input)\n",
    "            fake_pred = self.discriminator(fake_input)\n",
    "            \n",
    "            loss_D_real = self.criterion_gan(real_pred, torch.ones_like(real_pred))\n",
    "            loss_D_fake = self.criterion_gan(fake_pred, torch.zeros_like(fake_pred))\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "            \n",
    "            # Pérdida del generador\n",
    "            loss_G_gan = self.criterion_gan(fake_pred, torch.ones_like(fake_pred))\n",
    "            loss_G_l1 = self.criterion_l1(fake_ab, ab_color) * self.config.lambda_l1\n",
    "            loss_G = loss_G_gan + loss_G_l1\n",
    "            \n",
    "            # Acumular métricas\n",
    "            metrics['loss_D'] += loss_D.item()\n",
    "            metrics['loss_G'] += loss_G.item()\n",
    "            metrics['loss_G_gan'] += loss_G_gan.item()\n",
    "            metrics['loss_G_l1'] += loss_G_l1.item()\n",
    "            \n",
    "            # Guardar samples (solo del primer batch)\n",
    "            if save_samples and epoch % self.config.sample_every == 0:\n",
    "                save_colorization_samples(\n",
    "                    gray,\n",
    "                    fake_ab,\n",
    "                    ab_color,\n",
    "                    labels,\n",
    "                    epoch,\n",
    "                    self.config.samples_dir,\n",
    "                    max_samples=self.config.num_samples\n",
    "                )\n",
    "                save_samples = False\n",
    "        \n",
    "        # Promediar métricas\n",
    "        num_batches = len(val_loader)\n",
    "        metrics = {k: v / num_batches for k, v in metrics.items()}\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: Optional[DataLoader] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ciclo completo de entrenamiento\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader de entrenamiento\n",
    "            val_loader: DataLoader de validación (opcional)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"INICIANDO ENTRENAMIENTO DE GAN PARA COLORIZACIÓN\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Device: {self.config.device}\")\n",
    "        print(f\"Épocas: {self.config.num_epochs}\")\n",
    "        print(f\"Batch size: {self.config.batch_size}\")\n",
    "        print(f\"Learning rate G: {self.config.lr_generator}\")\n",
    "        print(f\"Learning rate D: {self.config.lr_discriminator}\")\n",
    "        print(f\"Scheduler: {self.config.scheduler_type if self.config.use_scheduler else 'None'}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Guardar configuración\n",
    "        self.config.save(os.path.join(self.config.logs_dir, \"config.json\"))\n",
    "        \n",
    "        # Intentar cargar checkpoint previo\n",
    "        self.load_checkpoint()\n",
    "        \n",
    "        # Loop de entrenamiento\n",
    "        for epoch in range(self.start_epoch, self.config.num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # ==========================================\n",
    "            # ENTRENAMIENTO\n",
    "            # ==========================================\n",
    "            train_metrics = self.train_epoch(train_loader, epoch)\n",
    "            \n",
    "            # Log de métricas\n",
    "            self.metrics_tracker.update(epoch, train_metrics, phase='train')\n",
    "            \n",
    "            # ==========================================\n",
    "            # VALIDACIÓN\n",
    "            # ==========================================\n",
    "            if val_loader is not None and epoch % self.config.validate_every == 0:\n",
    "                val_metrics = self.validate(val_loader, epoch)\n",
    "                self.metrics_tracker.update(epoch, val_metrics, phase='val')\n",
    "                \n",
    "                # Verificar si es el mejor modelo\n",
    "                is_best = self.metrics_tracker.is_best_model(val_metrics['loss_G'])\n",
    "            else:\n",
    "                val_metrics = None\n",
    "                is_best = False\n",
    "            \n",
    "            # ==========================================\n",
    "            # LEARNING RATE SCHEDULING\n",
    "            # ==========================================\n",
    "            if self.scheduler_G is not None:\n",
    "                if self.config.scheduler_type == \"plateau\" and val_metrics is not None:\n",
    "                    self.scheduler_G.step(val_metrics['loss_G'])\n",
    "                    self.scheduler_D.step(val_metrics['loss_D'])\n",
    "                else:\n",
    "                    self.scheduler_G.step()\n",
    "                    self.scheduler_D.step()\n",
    "            \n",
    "            # Log learning rates\n",
    "            current_lr_G = self.optim_G.param_groups[0]['lr']\n",
    "            current_lr_D = self.optim_D.param_groups[0]['lr']\n",
    "            self.metrics_tracker.log_learning_rates(epoch, current_lr_G, current_lr_D)\n",
    "            \n",
    "            # ==========================================\n",
    "            # CHECKPOINTING\n",
    "            # ==========================================\n",
    "            if (epoch + 1) % self.config.save_every == 0:\n",
    "                all_metrics = train_metrics.copy()\n",
    "                if val_metrics:\n",
    "                    all_metrics.update({f\"val_{k}\": v for k, v in val_metrics.items()})\n",
    "                \n",
    "                self.checkpoint_manager.save_checkpoint(\n",
    "                    epoch=epoch,\n",
    "                    generator=self.generator,\n",
    "                    discriminator=self.discriminator,\n",
    "                    optim_G=self.optim_G,\n",
    "                    optim_D=self.optim_D,\n",
    "                    scheduler_G=self.scheduler_G,\n",
    "                    scheduler_D=self.scheduler_D,\n",
    "                    metrics=all_metrics,\n",
    "                    is_best=is_best\n",
    "                )\n",
    "            \n",
    "            # ==========================================\n",
    "            # RESUMEN DE ÉPOCA\n",
    "            # ==========================================\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Época {epoch}/{self.config.num_epochs - 1} completada en {epoch_time:.2f}s\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"TRAIN | D: {train_metrics['loss_D']:.4f} | \"\n",
    "                  f\"G: {train_metrics['loss_G']:.4f} | \"\n",
    "                  f\"G_gan: {train_metrics['loss_G_gan']:.4f} | \"\n",
    "                  f\"G_l1: {train_metrics['loss_G_l1']:.4f}\")\n",
    "            \n",
    "            if val_metrics:\n",
    "                print(f\"VAL   | D: {val_metrics['loss_D']:.4f} | \"\n",
    "                      f\"G: {val_metrics['loss_G']:.4f} | \"\n",
    "                      f\"G_gan: {val_metrics['loss_G_gan']:.4f} | \"\n",
    "                      f\"G_l1: {val_metrics['loss_G_l1']:.4f}\")\n",
    "                if is_best:\n",
    "                    print(\"★ ¡NUEVO MEJOR MODELO! ★\")\n",
    "            \n",
    "            print(f\"LR    | G: {current_lr_G:.2e} | D: {current_lr_D:.2e}\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # ==========================================\n",
    "        # FIN DEL ENTRENAMIENTO\n",
    "        # ==========================================\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ENTRENAMIENTO COMPLETADO\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Guardar historial y gráficas finales\n",
    "        history_path = os.path.join(self.config.logs_dir, \"training_history.json\")\n",
    "        self.metrics_tracker.save_history(history_path)\n",
    "        print(f\"✓ Historial guardado: {history_path}\")\n",
    "        \n",
    "        plots_path = os.path.join(self.config.logs_dir, \"training_plots.png\")\n",
    "        self.metrics_tracker.plot_history(plots_path)\n",
    "        \n",
    "        self.metrics_tracker.close()\n",
    "        print(\"\\n¡Entrenamiento finalizado exitosamente! \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02603cbd",
   "metadata": {},
   "source": [
    "# 8. Función Principal\n",
    "\n",
    "## 8.1 Función main\n",
    "Punto de entrada del script, orquesta todo el proceso de entrenamiento\n",
    "\n",
    "### 8.1.1 Importación de Módulos\n",
    "Importa los modelos personalizados (generador, discriminador, dataset)\n",
    "\n",
    "### 8.1.2 Configuración del Experimento\n",
    "Crea y muestra la configuración del entrenamiento\n",
    "\n",
    "### 8.1.3 Preparación del Dataset\n",
    "Carga el dataset y lo divide en train/validation\n",
    "\n",
    "### 8.1.4 Creación de DataLoaders\n",
    "Configura los DataLoaders para train y validación\n",
    "\n",
    "### 8.1.5 Inicialización de Modelos\n",
    "Crea el generador y discriminador con sus pesos iniciales\n",
    "\n",
    "### 8.1.6 Creación del Trainer\n",
    "Instancia el GANTrainer con los modelos y configuración\n",
    "\n",
    "### 8.1.7 Ejecución del Entrenamiento\n",
    "Inicia el entrenamiento con manejo de interrupciones\n",
    "\n",
    "## 8.2 Entry Point\n",
    "Ejecuta la función main cuando se corre el script directamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604af787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Función principal para ejecutar el entrenamiento\"\"\"\n",
    "    \n",
    "    # Importar modelos (asumiendo que están disponibles)\n",
    "    try:\n",
    "        from Modelo.generador import ColorGenerator\n",
    "        from Modelo.discriminador import (\n",
    "            PatchGANDiscriminator,\n",
    "            init_discriminator_weights,\n",
    "            RECOMMENDED_CONFIG\n",
    "        )\n",
    "        from image_driver import ColorizationDataset\n",
    "    except ImportError as e:\n",
    "        print(f\"Error al importar módulos: {e}\")\n",
    "        print(\"Asegúrate de que los archivos estén en las rutas correctas.\")\n",
    "        return\n",
    "    \n",
    "    # ==========================================\n",
    "    # CONFIGURACIÓN\n",
    "    # ==========================================\n",
    "    config = TrainingConfig()\n",
    "    \n",
    "    # Opcional: modificar configuración desde aquí\n",
    "    # config.num_epochs = 50\n",
    "    # config.batch_size = 32\n",
    "    # config.lr_generator = 1e-4\n",
    "    \n",
    "    print(\"Configuración cargada:\")\n",
    "    print(json.dumps(config.to_dict(), indent=2))\n",
    "    \n",
    "    # ==========================================\n",
    "    # DATASET Y DATALOADERS\n",
    "    # ==========================================\n",
    "    print(\"\\nCargando datasets...\")\n",
    "    \n",
    "    # Dataset completo\n",
    "    full_dataset = ColorizationDataset(\n",
    "        config.data_dir,\n",
    "        num_classes=config.num_classes\n",
    "    )\n",
    "    \n",
    "    # Split train/val (80/20)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset,\n",
    "        [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Train samples: {len(train_dataset)}\")\n",
    "    print(f\"✓ Val samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # ==========================================\n",
    "    # MODELOS\n",
    "    # ==========================================\n",
    "    print(\"\\nInicializando modelos...\")\n",
    "    \n",
    "    # Generador\n",
    "    generator = ColorGenerator()\n",
    "    num_params_G = sum(p.numel() for p in generator.parameters())\n",
    "    print(f\"✓ Generador creado | Parámetros: {num_params_G:,}\")\n",
    "    \n",
    "    # Discriminador\n",
    "    discriminator = PatchGANDiscriminator(\n",
    "        input_channels=3,  # L + AB\n",
    "        features=64\n",
    "    )\n",
    "    init_discriminator_weights(discriminator)\n",
    "    num_params_D = discriminator.get_num_params()\n",
    "    print(f\"✓ Discriminador creado | Parámetros: {num_params_D:,}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # TRAINER\n",
    "    # ==========================================\n",
    "    trainer = GANTrainer(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # ==========================================\n",
    "    # ENTRENAR\n",
    "    # ==========================================\n",
    "    try:\n",
    "        trainer.train(train_loader, val_loader)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nEntrenamiento interrumpido por el usuario.\")\n",
    "        print(\"Guardando checkpoint final...\")\n",
    "        trainer.checkpoint_manager.save_checkpoint(\n",
    "            epoch=trainer.start_epoch,\n",
    "            generator=trainer.generator,\n",
    "            discriminator=trainer.discriminator,\n",
    "            optim_G=trainer.optim_G,\n",
    "            optim_D=trainer.optim_D,\n",
    "            scheduler_G=trainer.scheduler_G,\n",
    "            scheduler_D=trainer.scheduler_D,\n",
    "            metrics={},\n",
    "            is_best=False\n",
    "        )\n",
    "        print(\"✓ Checkpoint guardado. Puedes reanudar el entrenamiento más tarde.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
